{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobby-py2002/FoodVision/blob/main/scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZOWahzb22EA"
      },
      "source": [
        "scrape.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAXP7nfWPmYG",
        "outputId": "ac22404b-5408-4bcb-e63c-7e05a96a2301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    return ' '.join(text.split()).replace('\\n', ' ').strip()\n",
        "\n",
        "def scrape_recipe(url):\n",
        "  try:\n",
        "    response = requests.get(url , timeout = 30)\n",
        "    response.raise_for_status()\n",
        "    print(response.raise_for_status())\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    name = (soup.find('h1', class_= 'recipe-title')\n",
        "            or\n",
        "            soup.select_one('h1[data-testid=\"recipe-title\"]')\n",
        "            or\n",
        "            soup.find('h1')\n",
        "            )\n",
        "    if name:\n",
        "      name = name.text.strip()\n",
        "\n",
        "    ingredients = [clean_text(ing.text) for ing in soup.select('.ingredient-list li')]\n",
        "    instructions = [clean_text(ins.text) for ins in soup.select('.direction-list li')]\n",
        "    return {\n",
        "        'name': name,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions\n",
        "    }\n",
        "  except requests.exceptions.Timeout:\n",
        "     print(f\"üïí Timeout: {url} (server too slow)\")\n",
        "     return None\n",
        "  except requests.exceptions.HTTPError as e:\n",
        "    print(f\"üö® HTTP {e.response.status_code} error: {url}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"üí• Unexpected error on {url}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "recipes = []\n",
        "urls = ['https://www.food.com/recipe/bagel-french-toast-casserole-362199']\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        recipes.append(scrape_recipe(url))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "with open('/home/recipes.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(recipes, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEmjJOLZnf9Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def get_recipe_links():\n",
        "  try:\n",
        "    response = requests.get(\"https://www.food.com/ideas/top-breakfast-recipes-6935#c-796349\")\n",
        "    soup=BeautifulSoup(response.text, 'html.parser')\n",
        "    return list({\n",
        "        a['href'] for a in soup.find_all('a',href=True)\n",
        "        if '/recipe/' in a['href'].lower()}\n",
        "    )\n",
        "  except Exception as e:\n",
        "    print(f\"LINK ERROR : {e}\")\n",
        "\n",
        "with open('/home/links.json','w', encoding='utf-8') as l:\n",
        "  json.dump(get_recipe_links(),l,indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwSveqM_wOxI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import gzip\n",
        "import io\n",
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm  # Progress bar library\n",
        "\n",
        "# Configuration\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Accept-Encoding': 'gzip'\n",
        "}\n",
        "TEST_SITEMAPS = [\n",
        "    \"https://www.food.com/sitemap-1.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-2.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-3.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-4.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-5.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-6.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-7.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-8.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-9.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-10.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-11.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-12.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-13.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-14.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-15.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-16.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-17.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-18.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-19.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-20.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-21.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-22.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-23.xml.gz\",\n",
        "    \"https://www.food.com/sitemap-24.xml.gz\"\n",
        "]\n",
        "REQUEST_TIMEOUT = (3, 5)  # 3s connect, 5s read\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean recipe text\"\"\"\n",
        "    return ' '.join(text.split()).replace('\\n', ' ').strip() if text else \"\"\n",
        "\n",
        "def extract_urls(sitemap_url):\n",
        "    \"\"\"Get recipe URLs from a single sitemap\"\"\"\n",
        "    try:\n",
        "        response = requests.get(sitemap_url, headers=HEADERS, timeout=10)\n",
        "        with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz_file:\n",
        "            xml_content = gz_file.read()\n",
        "            root = ET.fromstring(xml_content)\n",
        "            return [elem.text for elem in root.iter()\n",
        "                   if elem.text and \"/recipe/\" in elem.text]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Sitemap error ({sitemap_url}): {type(e).__name__}\")\n",
        "        return []\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    \"\"\"Scrape a single recipe with fault tolerance\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "        # Extract data with fallbacks\n",
        "        name = (soup.find('h1', class_='recipe-title') or\n",
        "               soup.select_one('h1[data-testid=\"recipe-title\"]') or\n",
        "               soup.find('h1'))\n",
        "        name = clean_text(name.get_text()) if name else \"Untitled Recipe\"\n",
        "\n",
        "        ingredients = []\n",
        "        for selector in ['.structured-ingredients__list li', '.ingredient-list li']:\n",
        "            if not ingredients:\n",
        "                ingredients = [clean_text(ing.get_text())\n",
        "                             for ing in soup.select(selector)]\n",
        "\n",
        "        instructions = []\n",
        "        for selector in ['.recipe__steps-content li', '.direction-list li']:\n",
        "            if not instructions:\n",
        "                instructions = [clean_text(step.get_text())\n",
        "                              for step in soup.select(selector)]\n",
        "\n",
        "        return {\n",
        "            'name': name,\n",
        "            'url': url,\n",
        "            'ingredients': ingredients,\n",
        "            'instructions': instructions\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    print(\"üöÄ Starting scrape...\")\n",
        "\n",
        "    # Step 1: Get URLs from sitemaps (parallel)\n",
        "    print(\"üîç Extracting recipe URLs from sitemaps...\")\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        all_urls = list(set().union(*executor.map(extract_urls, TEST_SITEMAPS)))\n",
        "\n",
        "    print(f\"üìä Found {len(all_urls)} recipes to scrape\")\n",
        "\n",
        "    # Step 2: Scrape recipes with progress bar\n",
        "    recipes = []\n",
        "    failed = 0\n",
        "    with tqdm(total=len(all_urls), desc=\"Scraping Recipes\") as pbar:\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = []\n",
        "            for url in all_urls:\n",
        "                future = executor.submit(scrape_recipe, url)\n",
        "                future.add_done_callback(lambda _: pbar.update(1))\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in futures:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    recipes.append(result)\n",
        "                else:\n",
        "                    failed += 1\n",
        "                pbar.set_postfix({\"Success\": len(recipes), \"Failed\": failed})\n",
        "\n",
        "    # Save results\n",
        "    if recipes:\n",
        "        with open('/home/recipes.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(recipes, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n‚ú® Saved {len(recipes)} recipes ({failed} failed)\")\n",
        "    else:\n",
        "        print(\"\\nüí• All recipes failed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lxd-QrwTpfiXfjGmdIJQhV7zCt0M4waZ",
      "authorship_tag": "ABX9TyPJkWFoklo7u89+yWMjx5Hu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}